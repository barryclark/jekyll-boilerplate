---	
css: post.css
layout: post	
title: AICVS 1.0 - Text Cleaning, Text Augmentation and Transfer Learning in NLP
---	

AICVS concluded it’s webinar series AICVS 1.0,  with its 6th session on 14th July, 2020, on "Text Cleaning, text augmentation and transfer learning in NLP" . The speaker for the webinar was Meghana Bhange, the founder and the former Head Coordinator of AICVS Cummins.

Meghana is a 2020 EnTC graduate, from Cummins. She has worked on various projects, and internships focused on machine learning, from the beginning of her second year. Meghana interned at Verloop.io in her third year, where she started her NLP journey. She has previously given talks at PyData on custom name extractor for chat based conversations and DSC HIT on Arctic Monkeys Lyrics Generator. She is 12th on the global leaderboard for SemEval’s sentimix task. She loves to work with state of the art NLP ideas and push them into production. She continues to work at Verloop presently.

This session was a hands-on experience from text cleaning to transfer learning NLP models, and was done using Python. It began with a brief introduction to NLP. NLP is an abbreviation for Natural Language Processing. It deals with processing and analysing natural language data.

#### Collecting data
Getting started, the first step is to collect some text data. Meghana used song lyrics for building this model. For this you can use LyricsGenius, it has an api and a python package which let’s you query lyrics based on the Artist’s name or based on the albums.
You can visit “here” to get your API clients and a client secret (or the API key)
To obtain the lyrics, we require two things, one is the API key, and the Artist.

```sh
api=genius.Genius(API_KEY)
artist = api.search_artist(ARTIST)
print(artist.songs)
```
Next we save the lyrics  in a json file. The lyrics are stored in a dictionary format.
```
artist.save_lyrics()
```

To access the file, import json, open the file, and load it on some variable. Use var_name.keys()  to view all the keys the dictionary has.
```
import json
with open(“Lyrics_OriginalBroadwayCast ofHamilton.json”) as f:
	data = json.load(f)
data.keys()
```
> Insert Output Image


We want to use the “songs” key from the dictionary.
This can be accessed using
```
data.[“songs”]
```

To print the length, use :
```
len(data.[“songs”])
```

To view the content in one of the songs, you can query it by it’s index. For example:
```
data[“songs”][0]
```

>> Insert Output Image

Or we can further access the contents using keys. To navigate,
```
data[“songs”][0].keys()
```

>> Insert Output Image

The lyrics can obtained by using the lyrics key for the particular song :
```
data[“songs”][0][“lyrics”]
```

  - Type some Markdown on the left
  - See HTML in the right
  - Magic
#### Data cleaning
Pandas library is used widely for handling data. Import the library :
```
import pandas as pd
```

Create a dataframe to store the title of the song and its lyrics.
```
df=pd.DataFrame(data[“songs”])([“title”][“lyrics”])
```

To view the first 5 values in the dataframe, use :
```
df.head()
```

>> Insert Output Image

As we can see, the lyrics contain characters like \n. We need to clean the data.
```
df.dropna(inplace=True)
```
 “dropna” drops all the files which have nothing in them. We want everything in the data to be strings.

>> Insert Output Image

#### Lambda Functions:

```
df.[“lyrics”]
```
This returns a list of all lyrics that we have. To capitilse every sentence, a function can be written:
```
def make_title(sent):
 	return sent.title()
```
When we use “apply” to a dataframe column, it applies the specified function to that column.
```
df.[“lyrics”] .apply(make_title)
```

We can use this while cleaning the data, to remove unnecessary symbols etc.
For example, Meghana used :
```
df[“lyrics”] =(
    	df[“lyrics”]
    	.apply(lambda x: x.lower())
    	.apply(lambda x: x.replace("\\"," "))
    	.apply(lambda x: x.replace("\\n", " "))
    	.apply(lambda x: x.replace("\n", " "))
    	.apply(lambda x: x.replace("'", " "))
)
```
Regex can also be used if you want to remove complicated patterns of symbols

#### Data Augmentation
This technique is used to generate more data when we don’t have sufficient data.
A library called nlpaug serves this purpose.
Meghana demonstrated the augmentation:
```
text= 'I may or maynot graduate without giving exams'
aug=naw.SynonymAug(aug_src='wordnet')
augmented_text= aug.augment(text)
print(Original:")
print(text)
print("Augmented Text:")
print(augmented_text)
```
SynonymAug takes a word from the original sentence and replaces it with a nearest synonym from the wordnet. Output:
> Insert Output Image

On swapping randomly:
```
# @title Test Contextual WordEmbs Augmentation
aug=nafc.Sequential(
	[
    	naw.RandomWordAug(action=Action.SWAP),
    	naw.RandomWordAug(aug_src='wordnet'),
    	naw.RandomWordAug()
	]
)
test_text = "there is always somebody taller with more of a wit"
aug.augment(test_text)
```
Meghana suggests that this should be done in a sequential order. She wrote a logic to split the sentence after every 10 words, augment it, and store it in a text file.
```
from progressbar import PregressBar
pbar = PregressBar
augmented = []
for s in pbar(splitter(10,text)):
	try:
    	augmented.append(aug.augment(s))
	except ValueError:
    	pass
```
After obtaining the lyrics, they are given to a language model to generate the next word predictor.
# Transfer Learning

Meghana then briefed about Transfer learning. When we have a small amount of data containing just a few lines,  we can’t build a good model from scratch. Instead, we can take a model that has already been trained, use it’s parameters as starting weights for our task. She advises to explore this field further!

Here are some the lyrics her model generated :
 Insert Output Image
She concluded the session by giving some advice for some basic NLP projects ideas, such as building a classifier for classifying data into toxic or nontoxic etc. , or try sentiment analysis on twitter data.


Link to Webinar:

[![Webinar 6](http://img.youtube.com/vi/O1OiagKy1-A/0.jpg)](http://www.youtube.com/watch?v=O1OiagKy1-A "Webinar 6")

- - -

### *Written by Shreya Hardas*

